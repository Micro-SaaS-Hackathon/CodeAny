GOOGLE_OAUTH_ID=""
GOOGLE_OAUTH_SECRET=""
SUPABASE_API_KEY=""
SUPABASE_PROJECT_URL=""

# FastAPI backend URL (used by frontend)
BACKEND_URL="http://localhost:8000"

# Convex configuration (optional for local fallback)
# Set CONVEX_URL to your deployment URL from Convex dashboard, e.g. https://my-space-123.convex.cloud
# If you have a .convex.site address, the client will also try .convex.cloud automatically.
CONVEX_URL=""
# Optional: admin deploy key for streaming APIs; not required for Functions API
CONVEX_DEPLOY_KEY=""
# Optional: user bearer token to call user-authenticated functions
CONVEX_USER_BEARER=""

# CORS origin for backend
FRONTEND_ORIGIN="http://localhost:3010"

# OpenRouter (all LLM calls except image generation)
OPENROUTER_API_KEY=""

# Google AI Studio (Gemini API) â€” ONLY for gemini-2.5-flash-image-preview
GEMINI_API_KEY=""
# or GOOGLE_API_KEY=""

# App name (OpenRouter attribution headers)
APP_URL="http://localhost:8000"
APP_TITLE="Cursly CourseFactory"

# Default models (Kimi-first)
SYLLABUS_MODEL="moonshotai/kimi-k2:free"
TEXT_MODEL="moonshotai/kimi-k2:free"
MANIM_MODEL="deepseek/deepseek-coder"
# Kept for compatibility. Image generation is disabled; we produce Q&A instead.
GEMINI_MODEL="moonshotai/kimi-k2:free"
GEMINI_IMAGE_MODEL="gemini-2.5-flash-image-preview"

# Rate limits and concurrency (optional)
# Max retries for OpenRouter and Gemini calls (on 429/5xx/timeouts)
LLM_MAX_RETRIES="5"
GEMINI_MAX_RETRIES="5"
# Bounded concurrency for provider calls
LLM_CONCURRENCY="3"
GEMINI_CONCURRENCY="2"

# AI logging
AI_LOG_LEVEL="INFO"     # DEBUG|INFO|WARNING|ERROR
AI_LOG_PREVIEW="240"     # max characters to preview for model outputs/log entries
AI_LOG_FILE="logs/ai.log"  # file to persist AI logs (rotates at ~5MB x3)

# Manim Docker runner (optional tuning)
MANIM_DOCKER_IMAGE="manimcommunity/manim:stable"
# Set to "none" to disable mapping host uid:gid into container (may help on some Docker setups)
MANIM_DOCKER_USER="host"
# Where to place temp workdirs for Manim (must be a path Docker can mount).
# Recommendation: use a path OUTSIDE the repo to avoid dev reload loops, e.g.:
#   MANIM_TMP_DIR="$HOME/.cache/cursly/manim_runs"
# If unset, defaults to ~/.cache/cursly/manim_runs
MANIM_TMP_DIR=""
MANIM_MAX_PARALLEL="2"               # max concurrent docker runs for manim
MANIM_DOCKER_INSPECT_TIMEOUT_S="8"   # timeout for docker image inspect
MANIM_DOCKER_PULL_TIMEOUT_S="120"    # timeout for docker pull
MANIM_DOCKER_RUN_TIMEOUT_S="180"     # timeout for docker run

# Manim local execution
# Prefer local Manim (current Python environment) before Docker. Default: 1
MANIM_LOCAL_FIRST="1"
# Force enable/disable backends (both can be 1; order controlled by MANIM_LOCAL_FIRST)
MANIM_ENABLE_DOCKER="1"
MANIM_ENABLE_LOCAL="1"
# Optional: explicitly set the Python interpreter or manim binary for local runs
# MANIM_LOCAL_PYTHON="/path/to/venv/bin/python"
# MANIM_LOCAL_BIN="/path/to/venv/bin/manim"
# If MANIM_LOCAL_PYTHON or MANIM_LOCAL_BIN are relative (e.g., .venv/bin/python),
# they are resolved against MANIM_BASE_DIR if set, else the backend's launch CWD.
# MANIM_BASE_DIR="/absolute/path/to/project/root"

# Placeholder video fallback (used only if both Docker and local fail)
MANIM_ENABLE_PLACEHOLDER="1"
MANIM_PLACEHOLDER_SECONDS="5"
MANIM_PLACEHOLDER_SIZE="1280x720"

# pydub/ffmpeg detection (used by Manim audio pipeline)
# If pydub warns about ffmpeg, set this explicitly (inside Docker we default to 'ffmpeg').
# FFMPEG_BINARY="ffmpeg"
